{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Reinforcement Learning - Gridworld.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W9FdYDSqzVU",
        "colab_type": "text"
      },
      "source": [
        "## Off policy Q-learning and Sarsa algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wXSwlajx0y7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "class Environment():\n",
        "    \n",
        "    def __init__(self, epsilon=0.9, episodes=100, max_steps=100, alpha=0.085, gamma=0.9):\n",
        "        self.epsilon = epsilon      # e-greedy\n",
        "        self.num_episodes = episodes\n",
        "        self.max_steps = max_steps\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.state = None\n",
        "        self.reward = 0\n",
        "        self.state_space = 5*4\n",
        "        self.action_space = 4\n",
        "        self.Q = np.zeros((self.state_space, self.action_space), np.float32)\n",
        "\n",
        "    def next_action(self, state, exploitation=False):       \n",
        "        if np.random.uniform(0,1) < self.epsilon and exploitation == False: \n",
        "            action = np.random.randint(self.action_space) # Explore\n",
        "        else: \n",
        "            action = np.argmax(self.Q[state, :])          # Exploit\n",
        "        return action \n",
        "  \n",
        "    def next_state(self, state, action):\n",
        "        x = int(state/5)\n",
        "        y = state%5\n",
        "    \n",
        "        old_x,old_y = x,y\n",
        "            \n",
        "        # Set up Next-State\n",
        "        if (action == 0):   # Up\n",
        "            if (x != 0):\n",
        "                x = x-1\n",
        "        \n",
        "        elif (action == 1): # Right\n",
        "            if (y != 4):\n",
        "                y = y+1\n",
        "        \n",
        "        elif (action == 2): # Down\n",
        "            if (x != 3):\n",
        "                x = x+1      \n",
        "        \n",
        "        elif (action == 3): # Left\n",
        "            if (y != 0):\n",
        "                y = y-1\n",
        "                                        \n",
        "        self.state = x*5+y\n",
        "        \n",
        "        # Set up Reward according to rules\n",
        "        if ((x,y) == (old_x,old_y)): # Off-Grid\n",
        "            reward = -1\n",
        "        elif ((x,y) == (0,0)):       # Goal-State\n",
        "            reward = 10\n",
        "        elif ((x,y) == (1,3)):       # F-Position\n",
        "            reward = -5\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        if ((old_x,old_y) == (1,3)):       # F-Position\n",
        "            self.state_visualization[old_x][old_y] = \"F\"\n",
        "        else:\n",
        "            self.state_visualization[old_x][old_y] = \"-\"\n",
        "        self.state_visualization[x][y] = \"S\"\n",
        "\n",
        "        return self.state, reward\n",
        "\n",
        "    def updateQ(self, current_state, next_state, reward, current_action, next_action):\n",
        "        Q = self.Q[current_state][current_action] \n",
        "        Qnext = (reward + self.gamma * self.Q[next_state][next_action])\n",
        "        self.Q[current_state][current_action] = (1-self.alpha)*Q + self.alpha*Qnext\n",
        "        \n",
        "    def reset(self):\n",
        "        self.state_visualization = np.array([['G', '-' ,'-' ,'-', '-'],\n",
        "                              ['-', '-' ,'-' ,'F', '-'],\n",
        "                              ['-', '-' ,'-' ,'-', '-'],\n",
        "                              ['-', '-' ,'-' ,'-', 'S']])\n",
        "        # Initial State ('S')\n",
        "        self.state = 19\n",
        "        return self.state\n",
        "        \n",
        "    def is_goal_state(self):\n",
        "        return self.state == 0\n",
        "    \n",
        "    def actions_map(self, i):\n",
        "        if (i==0):\n",
        "            return \"Up\"\n",
        "        elif (i==1):\n",
        "            return \"Right\"\n",
        "        elif (i==2):\n",
        "            return \"Down\"\n",
        "        elif (i==3):\n",
        "            return \"Left\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5jKJ78A0y7o",
        "colab_type": "text"
      },
      "source": [
        "### Q-Learning Algorithm (off-policy TD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTgRyZHb0y7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q Algorithm (off-policy TD)\n",
        "\n",
        "# Generate the Environment\n",
        "environment = Environment(episodes=300, max_steps=100)\n",
        "\n",
        "for episode in range(environment.num_episodes): \n",
        "    t = 0\n",
        "    \n",
        "    # Initialize S\n",
        "    current_state = environment.reset()\n",
        "\n",
        "    # Repeat for each step of episode\n",
        "    while t < environment.max_steps and not environment.is_goal_state(): \n",
        "        \n",
        "        # Select 'a' using policy derived from Q (e-greedy)\n",
        "        current_action = environment.next_action(current_state) \n",
        "        \n",
        "        # Take action 'a', observe immediate reward and new state\n",
        "        next_state, reward = environment.next_state(current_state, current_action)\n",
        "\n",
        "         #print(environment.state_visualization, \",   \", \n",
        "         #      environment.actions_map(current_action), \n",
        "         #      \", reward:\", reward, \"\\n\\n\")\n",
        "\n",
        "        # Select next-action by using the exploitation flag to True so that we\n",
        "        # get the maximum Qnext in the update-rule\n",
        "        next_action = environment.next_action(next_state, exploitation=True)\n",
        "        \n",
        "        # Update Q\n",
        "        environment.updateQ(current_state, next_state, reward, current_action, next_action)\n",
        "  \n",
        "        current_state = next_state\n",
        "          \n",
        "        t += 1\n",
        "        \n",
        "# print(\"\\n\\n\", environment.Q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3k65xly0y7r",
        "colab_type": "text"
      },
      "source": [
        "### Play once in deterministic mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkKP4NEi0y7s",
        "colab_type": "code",
        "outputId": "65a28ad4-941f-45bf-d8ea-8ae0382b75c5",
        "colab": {}
      },
      "source": [
        "# Play Once\n",
        "exploitation = True\n",
        "current_state = environment.reset()\n",
        "current_action = environment.next_action(current_state, exploitation) \n",
        "  \n",
        "# Repeat until agent reaches goal state or max_steps\n",
        "while t < environment.max_steps and not environment.is_goal_state():\n",
        "\n",
        "    next_state, reward = environment.next_state(current_state, current_action)\n",
        "\n",
        "    print(environment.state_visualization, \",   \", \n",
        "          environment.actions_map(current_action), \n",
        "          \", reward:\", reward, \"\\n\\n\")\n",
        "\n",
        "    next_action = environment.next_action(next_state, exploitation)\n",
        "\n",
        "    current_state = next_state\n",
        "    current_action = next_action\n",
        "\n",
        "    t += 1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['G' '-' '-' '-' '-']\n",
            " ['-' '-' '-' 'F' '-']\n",
            " ['-' '-' '-' '-' 'S']\n",
            " ['-' '-' '-' '-' '-']] ,    Up , reward: 0 \n",
            "\n",
            "\n",
            "[['G' '-' '-' '-' '-']\n",
            " ['-' '-' '-' 'F' 'S']\n",
            " ['-' '-' '-' '-' '-']\n",
            " ['-' '-' '-' '-' '-']] ,    Up , reward: 0 \n",
            "\n",
            "\n",
            "[['G' '-' '-' '-' 'S']\n",
            " ['-' '-' '-' 'F' '-']\n",
            " ['-' '-' '-' '-' '-']\n",
            " ['-' '-' '-' '-' '-']] ,    Up , reward: 0 \n",
            "\n",
            "\n",
            "[['G' '-' '-' 'S' '-']\n",
            " ['-' '-' '-' 'F' '-']\n",
            " ['-' '-' '-' '-' '-']\n",
            " ['-' '-' '-' '-' '-']] ,    Left , reward: 0 \n",
            "\n",
            "\n",
            "[['G' '-' 'S' '-' '-']\n",
            " ['-' '-' '-' 'F' '-']\n",
            " ['-' '-' '-' '-' '-']\n",
            " ['-' '-' '-' '-' '-']] ,    Left , reward: 0 \n",
            "\n",
            "\n",
            "[['G' 'S' '-' '-' '-']\n",
            " ['-' '-' '-' 'F' '-']\n",
            " ['-' '-' '-' '-' '-']\n",
            " ['-' '-' '-' '-' '-']] ,    Left , reward: 0 \n",
            "\n",
            "\n",
            "[['S' '-' '-' '-' '-']\n",
            " ['-' '-' '-' 'F' '-']\n",
            " ['-' '-' '-' '-' '-']\n",
            " ['-' '-' '-' '-' '-']] ,    Left , reward: 10 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d8M9WAJ0y7w",
        "colab_type": "code",
        "outputId": "b4df38cb-a5e9-42fa-e950-6f3d40ff7582",
        "colab": {}
      },
      "source": [
        "print(\"Rows: States [0,1,2,..,19]\")\n",
        "print(\"Columns: Up, Right, Down, Left\")\n",
        "print(environment.Q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Rows: States [0,1,2,..,19] \n",
            "Columns: Up, Right, Down, Left\n",
            "\n",
            "[[ 0.     0.     0.     0.   ]\n",
            " [ 7.999  8.1    8.099 10.   ]\n",
            " [ 7.1    7.29   7.29   9.   ]\n",
            " [ 6.29   6.561  1.56   8.1  ]\n",
            " [ 5.56   5.561  5.904  7.29 ]\n",
            " [ 9.998  8.058  7.7    7.818]\n",
            " [ 9.     7.29   7.285  8.991]\n",
            " [ 8.1    1.561  6.56   8.1  ]\n",
            " [ 7.29   5.902  5.902  7.29 ]\n",
            " [ 6.561  4.904  5.313  1.561]\n",
            " [ 8.986  7.242  7.003  6.894]\n",
            " [ 8.1    6.544  6.495  8.055]\n",
            " [ 7.29   5.903  5.892  7.287]\n",
            " [ 1.561  5.312  5.308  6.561]\n",
            " [ 5.905  4.312  4.781  5.905]\n",
            " [ 8.061  6.349  5.936  6.054]\n",
            " [ 7.27   5.843  5.441  7.117]\n",
            " [ 6.56   5.277  4.863  6.486]\n",
            " [ 5.904  4.778  4.306  5.896]\n",
            " [ 5.314  3.782  3.782  5.313]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-xLK8Ss0y7z",
        "colab_type": "code",
        "outputId": "fed7f734-a889-48c9-e488-b1c9ed40ee55",
        "colab": {}
      },
      "source": [
        "print(\"Values-States [4,5]\")\n",
        "print(np.reshape(np.max(environment.Q, axis=1), [4, 5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Values-States [4,5]\n",
            "[[ 0.    10.     4.298  0.455 -2.083]\n",
            " [10.     4.016  2.019 -2.162 -2.995]\n",
            " [ 3.939  1.197  0.325 -0.744 -2.205]\n",
            " [ 1.348  0.055 -0.83  -0.836 -1.662]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIHqV9VV0y73",
        "colab_type": "text"
      },
      "source": [
        "### SARSA Algorithm (on-policy TD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3dVJ49p0y73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SARSA Algorithm (on-policy TD)\n",
        "\n",
        "# Generate the Environment\n",
        "environment = Environment(episodes=300, max_steps=100)\n",
        "\n",
        "for episode in range(environment.num_episodes): \n",
        "    t = 0\n",
        "    \n",
        "    # Initialize S\n",
        "    current_state = environment.reset()\n",
        "    # Select 'a' using policy derived from Q (e-greedy)\n",
        "    current_action = environment.next_action(current_state) \n",
        "  \n",
        "    # Repeat for each step of episode\n",
        "    while t < environment.max_steps and not environment.is_goal_state(): \n",
        "        \n",
        "        # Take action 'a', observe immediate reward and new state\n",
        "        next_state, reward = environment.next_state(current_state, current_action)\n",
        "\n",
        "         #print(environment.state_visualization, \",   \", \n",
        "         #      environment.actions_map(current_action), \n",
        "         #      \", reward:\", reward, \"\\n\\n\")\n",
        "\n",
        "        # Select next action using policy derived from Q (e-greedy)\n",
        "        next_action = environment.next_action(next_state)\n",
        "\n",
        "        # Update Q\n",
        "        environment.updateQ(current_state, next_state, reward, current_action, next_action)\n",
        "  \n",
        "        current_state = next_state\n",
        "        current_action = next_action\n",
        "          \n",
        "        t += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2DIem-H0y75",
        "colab_type": "text"
      },
      "source": [
        "### Play once in deterministic mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHFXIGlr0y76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Play Once\n",
        "exploitation = True\n",
        "current_state = environment.reset()\n",
        "current_action = environment.next_action(current_state, exploitation) \n",
        "  \n",
        "# Repeat until agent reaches goal state or max_steps\n",
        "while t < environment.max_steps and not environment.is_goal_state():\n",
        "\n",
        "    next_state, reward = environment.next_state(current_state, current_action)\n",
        "\n",
        "    print(environment.state_visualization, \",   \", \n",
        "          environment.actions_map(current_action), \n",
        "          \", reward:\", reward, \"\\n\\n\")\n",
        "\n",
        "    next_action = environment.next_action(next_state, exploitation)\n",
        "\n",
        "    current_state = next_state\n",
        "    current_action = next_action\n",
        "\n",
        "    t += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqDEn6hX0y79",
        "colab_type": "code",
        "outputId": "18980ed0-9e36-4d78-9dd7-092615109fe9",
        "colab": {}
      },
      "source": [
        "print(\"\\nRows: States [0,1,2,..,19] \\nColumns: Up, Right, Down, Left\\n\")\n",
        "print(environment.Q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Rows: States [0,1,2,..,19] \n",
            "Columns: Up, Right, Down, Left\n",
            "\n",
            "[[ 0.     0.     0.     0.   ]\n",
            " [ 2.788  0.246  2.089 10.   ]\n",
            " [-1.878 -2.618  0.078  4.298]\n",
            " [-3.683 -3.041 -7.041  0.455]\n",
            " [-3.929 -3.819 -3.6   -2.083]\n",
            " [10.     1.231  2.146  3.599]\n",
            " [ 2.775 -1.916  0.624  4.016]\n",
            " [ 0.396 -7.356 -0.91   2.019]\n",
            " [-2.998 -3.1   -2.162 -2.386]\n",
            " [-2.995 -4.715 -3.032 -7.296]\n",
            " [ 3.939  0.433 -0.222  1.189]\n",
            " [ 0.889 -0.642 -0.489  1.197]\n",
            " [-0.646 -3.077 -0.98   0.325]\n",
            " [-7.422 -2.719 -1.979 -0.744]\n",
            " [-4.12  -3.884 -2.205 -2.231]\n",
            " [ 1.348 -0.216 -1.033 -1.466]\n",
            " [ 0.055 -1.107 -1.702 -0.296]\n",
            " [-0.83  -1.915 -2.115 -0.856]\n",
            " [-3.44  -2.415 -3.046 -0.836]\n",
            " [-2.642 -3.307 -3.265 -1.662]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVrMPOsQ0y8A",
        "colab_type": "code",
        "outputId": "c93658e7-584e-492c-d91e-296ab83429ef",
        "colab": {}
      },
      "source": [
        "print(\"Values-States [4,5]\")\n",
        "print(np.reshape(np.max(environment.Q, axis=1), [4, 5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Values-States [4,5]\n",
            "[[ 0.    10.     4.298  0.455 -2.083]\n",
            " [10.     4.016  2.019 -2.162 -2.995]\n",
            " [ 3.939  1.197  0.325 -0.744 -2.205]\n",
            " [ 1.348  0.055 -0.83  -0.836 -1.662]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}